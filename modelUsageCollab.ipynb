{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyP7LnIAzec9PJ8pqTfNEleZ"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Architecture"
   ],
   "metadata": {
    "id": "qH2D0v5gEqTa",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "#output - vector of vectors, labels - vactor\n",
    "def accuracy(outputs, labels):\n",
    "    return torch.tensor(torch.sum(outputs.argmax(1) == labels.argmax(1)).item() / len(labels))\n",
    "\n",
    "#implement Base for Classification\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        #print(batch.shape)\n",
    "        #print()\n",
    "        #batch - (img, label)\n",
    "        self.train()\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        #batch - (img, label)\n",
    "        self.eval() \n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        #outputs - list of dicts of validations \n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(f\"Epoch {epoch+1}: train_loss: {result['train_loss']}, val_loss: {result['val_loss']}, val_acc: {result['val_acc']}\")\n",
    "\n",
    "class ImageClassificationInception(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        #print(batch.shape)\n",
    "        #print()\n",
    "        #batch - (img, label)\n",
    "        self.train() \n",
    "        images, labels = batch \n",
    "        outputs = self(images) \n",
    "        loss1 = F.cross_entropy(outputs, labels)\n",
    "        loss = loss1\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        #batch - (img, label)\n",
    "        self.eval() \n",
    "        images, labels = batch \n",
    "        outputs = self(images) \n",
    "        loss1 = F.cross_entropy(outputs, labels)\n",
    "        loss = loss1\n",
    "        acc = accuracy(outputs, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        #outputs - list of dicts of validations \n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(f\"Epoch {epoch+1}: train_loss: {result['train_loss']}, val_loss: {result['val_loss']}, val_acc: {result['val_acc']}\")"
   ],
   "metadata": {
    "id": "F2GE7z5FElOP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "classDecode = {1:\"paper/cardboard\", 2: \"metal\", 3: \"plastic\", 4: \"glass\"}\n",
    "class ResNet(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Use a pretrained model\n",
    "        self.network = models.resnet50(pretrained=True)\n",
    "        # Replace last layer\n",
    "        num_ftrs = self.network.fc.in_features\n",
    "        self.network.fc = nn.Linear(num_ftrs, len(classDecode.values()))\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        o = self.network(xb)\n",
    "        return F.softmax(o)\n",
    "\n",
    "class DenseNet(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = models.densenet121(pretrained=True)\n",
    "        self.network.classifier = nn.Linear(1024, len(classDecode.values()))\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        o = self.network(xb)\n",
    "        return F.softmax(o)\n",
    "\n",
    "class InceptionNet(ImageClassificationInception):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = models.inception_v3(pretrained=True)\n",
    "        self.network.aux_logits=False\n",
    "        num_ftrs = self.network.AuxLogits.fc.in_features\n",
    "        self.network.AuxLogits.fc = nn.Linear(num_ftrs, len(classDecode.values()))\n",
    "        num_ftrs = self.network.fc.in_features\n",
    "        self.network.fc = nn.Linear(num_ftrs,len(classDecode.values()))\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        o = self.network(xb)\n",
    "        return F.softmax(o)\n",
    "\n",
    "\n",
    "class VGGNet(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Use a pretrained model\n",
    "        self.network = models.vgg16(pretrained=True)\n",
    "        # Replace last layer\n",
    "        self.network.classifier[6] = torch.nn.Linear(4096, len(classDecode.values()))\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        o = self.network(xb)\n",
    "        return F.softmax(o)\n",
    "\n",
    "class Net(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=(1,1))\n",
    "        self.conv2 = nn.Conv2d(16, 16, 3, padding=(1,1))\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3, padding=(1,1))\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(64, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4 , 128)\n",
    "        self.fc3 = nn.Linear(128, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = self.pool(F.relu(self.conv5(x)))\n",
    "        x = self.pool(F.relu(self.conv6(x)))\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x)\n"
   ],
   "metadata": {
    "id": "Q36t2RysEocw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model loading"
   ],
   "metadata": {
    "id": "Q5UnqbnjEuGe",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n"
   ],
   "metadata": {
    "id": "CdxTtCkAE0Pt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import time\n",
    "\n",
    "def read_and_resize(filename, grayscale = False, fx= 1, fy=1):\n",
    "    #read file\n",
    "    if grayscale:\n",
    "        img_result = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "    else:\n",
    "        imgbgr = cv2.imread(filename, cv2.IMREAD_COLOR)\n",
    "        img_result = cv2.cvtColor(imgbgr, cv2.COLOR_BGR2RGB)\n",
    "    img_result = cv2.resize(img_result, None, fx=fx, fy=fy, interpolation = cv2.INTER_CUBIC)\n",
    "    return img_result\n",
    "\n",
    "def getImageFromFolderAndMove(pathToImage, pathNewFolderImages):\n",
    "    #get image and move produced image to another folder\n",
    "    out = []\n",
    "    if not os.path.isdir(pathNewFolderImages):\n",
    "        os.mkdir(pathNewFolderImages)\n",
    "    if len(os.listdir(pathToImage)) == 0:\n",
    "        print(\"Folder toSort is empty!\")\n",
    "    for filename in os.listdir(pathToImage):\n",
    "        imgPath = (pathToImage+\"/\").replace(\"//\", \"/\")+filename\n",
    "        out = read_and_resize(imgPath)\n",
    "        shutil.move(imgPath, (pathNewFolderImages+\"/\").replace(\"//\", \"/\")+filename)\n",
    "        return [out, filename]\n",
    "\n",
    "def resizeImage(img, size):\n",
    "    #reshape to the shape of (size, size)\n",
    "    return cv2.resize(img, (256,256), interpolation = cv2.INTER_AREA).astype(\"float32\")/255\n",
    "\n",
    "def convertImg(img, device):\n",
    "    #prepare image to pass it to the model\n",
    "    return torch.tensor([img.swapaxes(2,1).swapaxes(1,0)]).to(device)\n",
    "\n",
    "def passToModel(model, image):\n",
    "    return model(image).argmax(1)\n",
    "\n",
    "def loadModel(pathTo, device):\n",
    "    model = to_device(DenseNet(), device)\n",
    "    model.load_state_dict(torch.load(pathTo, map_location=device))\n",
    "    return model\n",
    "\n",
    "def logInfo(pathToCsv, imgName, predict, predictClass):\n",
    "    if not os.path.isfile(pathToCsv): \n",
    "        df = pd.DataFrame({'name': [],\n",
    "                   'class': [],\n",
    "                   'classname': []})\n",
    "    else:\n",
    "        df = pd.read_csv(pathToCsv)\n",
    "    new_row = pd.DataFrame({'name': [imgName],\n",
    "                   'class': [predict],\n",
    "                   'classname': [predictClass]})\n",
    "    df = df.append(new_row, ignore_index=True)\n",
    "    df.to_csv(pathToCsv, index=False)\n",
    "\n",
    "\n",
    "def getResizePass(pathToModel, pathToImage, pathNewFolderImages, csvFilePath, classDecode):\n",
    "    device = get_default_device()\n",
    "    model = loadModel(pathToModel, device)\n",
    "    model.eval()\n",
    "    img, filename = getImageFromFolderAndMove(pathToImage, pathNewFolderImages)\n",
    "    resImg = convertImg(resizeImage(img, 256), device)\n",
    "\n",
    "    out = passToModel(model, resImg)\n",
    "    cls = classDecode[int(out[0]+1)]\n",
    "    numCls = int(out[0]+1)\n",
    "    log = logInfo(csvFilePath, filename, numCls, cls)\n",
    "    print(f\"Model predicted class {numCls}, which is a {cls}\")\n",
    "    \n",
    "def main():\n",
    "    googlePath = \"./drive/MyDrive/\"\n",
    "    localPath = \"./processing/\"\n",
    "    path = googlePath\n",
    "    pathToModel = path + \"model/denceNetModel.pt\"  # pretrained model. It takes RGB images with size 256x256\n",
    "    imgPath = path + \"toSort\"  # folder with all images to sort\n",
    "    imgNewPath = path + \"sorted\"  # folder with all sorted images\n",
    "    csvPath = path + \"logGarbage.csv\"  # file for the logging\n",
    "    classDecoder = {1: \"paper/cardboard\", 2: \"metal\", 3: \"plastic\", 4: \"glass\"}  # all classes\n",
    "\n",
    "    start = time.time()\n",
    "    getResizePass(pathToModel, imgPath, imgNewPath, csvPath, classDecoder)\n",
    "    end = time.time()\n",
    "    print(f\"Time of execution is {end - start} seconds!\")"
   ],
   "metadata": {
    "id": "4W4E1qXtFk7D",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "jkmmi6sZEQgS",
    "outputId": "15c57fdb-c872-4271-e16b-f280dd780ed5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './drive/MyDrive/model/denceNetModel.pt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [4]\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     82\u001B[0m classDecoder \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m1\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpaper/cardboard\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m2\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetal\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m3\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mplastic\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m4\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mglass\u001B[39m\u001B[38;5;124m\"\u001B[39m}  \u001B[38;5;66;03m# all classes\u001B[39;00m\n\u001B[1;32m     84\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m---> 85\u001B[0m \u001B[43mgetResizePass\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpathToModel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimgPath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimgNewPath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcsvPath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclassDecoder\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     86\u001B[0m end \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTime of execution is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mend \u001B[38;5;241m-\u001B[39m start\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m seconds!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Input \u001B[0;32mIn [4]\u001B[0m, in \u001B[0;36mgetResizePass\u001B[0;34m(pathToModel, pathToImage, pathNewFolderImages, csvFilePath, classDecode)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgetResizePass\u001B[39m(pathToModel, pathToImage, pathNewFolderImages, csvFilePath, classDecode):\n\u001B[1;32m     62\u001B[0m     device \u001B[38;5;241m=\u001B[39m get_default_device()\n\u001B[0;32m---> 63\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mloadModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpathToModel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     64\u001B[0m     img, filename \u001B[38;5;241m=\u001B[39m getImageFromFolderAndMove(pathToImage, pathNewFolderImages)\n\u001B[1;32m     65\u001B[0m     resImg \u001B[38;5;241m=\u001B[39m convertImg(resizeImage(img, \u001B[38;5;241m256\u001B[39m), device)\n",
      "Input \u001B[0;32mIn [4]\u001B[0m, in \u001B[0;36mloadModel\u001B[0;34m(pathTo, device)\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mloadModel\u001B[39m(pathTo, device):\n\u001B[1;32m     43\u001B[0m     model \u001B[38;5;241m=\u001B[39m to_device(DenseNet(), device)\n\u001B[0;32m---> 44\u001B[0m     model\u001B[38;5;241m.\u001B[39mload_state_dict(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpathTo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/serialization.py:699\u001B[0m, in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001B[0m\n\u001B[1;32m    696\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    697\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m--> 699\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[1;32m    700\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[1;32m    701\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[1;32m    702\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[1;32m    703\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[1;32m    704\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/serialization.py:230\u001B[0m, in \u001B[0;36m_open_file_like\u001B[0;34m(name_or_buffer, mode)\u001B[0m\n\u001B[1;32m    228\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[0;32m--> 230\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    231\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    232\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/serialization.py:211\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[0;34m(self, name, mode)\u001B[0m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[0;32m--> 211\u001B[0m     \u001B[38;5;28msuper\u001B[39m(_open_file, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './drive/MyDrive/model/denceNetModel.pt'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n"
   ],
   "metadata": {
    "id": "iMpIjwBjMi2w",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 98,
   "outputs": []
  }
 ]
}