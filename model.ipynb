{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ILi8A2iKP1GS"
   },
   "outputs": [],
   "source": [
    "import tqdm as tqdm\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.layers.convolutional import *\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "from torchvision.transforms import transforms\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Collab code!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "s01rxZMQCTjW"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting kaggle\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "\u001B[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58 kB 3.8 MB/s \n",
      "\u001B[?25hBuilding wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73050 sha256=6d920347893a5d952507daa5df8120af5232821a17c859087ed6f614a6119068\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
      "Successfully built kaggle\n",
      "Installing collected packages: kaggle\n",
      "  Attempting uninstall: kaggle\n",
      "    Found existing installation: kaggle 1.5.12\n",
      "    Uninstalling kaggle-1.5.12:\n",
      "      Successfully uninstalled kaggle-1.5.12\n",
      "Successfully installed kaggle-1.5.12\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall --no-deps kaggle"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "sbcsPGmhCTjX",
    "outputId": "909c3eb1-68bb-4bb1-f2ed-3f691791943f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "jiO41KTACTjY",
    "outputId": "e47ddb8e-c940-4b64-fe1a-0076e7c8c56d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-37c414b2-552f-4ebc-8da2-e42fd2a6cd31\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-37c414b2-552f-4ebc-8da2-e42fd2a6cd31\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saving kaggle.json to kaggle.json\n",
      "ref                                                            title                                                size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
      "-------------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
      "akshaydattatraykhare/diabetes-dataset                          Diabetes Dataset                                      9KB  2022-10-06 08:55:25          15939        465  1.0              \n",
      "akshaydattatraykhare/data-for-admission-in-the-university      Data for Admission in the University                  4KB  2022-10-27 11:05:45           4035         90  1.0              \n",
      "thedevastator/jobs-dataset-from-glassdoor                      Salary Prediction                                     3MB  2022-11-16 13:52:31            606         24  1.0              \n",
      "swaptr/layoffs-2022                                            Layoffs 2022                                         30KB  2022-11-20 18:05:26           1218         40  1.0              \n",
      "meirnizri/covid19-dataset                                      COVID-19 Dataset                                      5MB  2022-11-13 15:47:17           1100         32  1.0              \n",
      "dheerajmukati/india-gdp-19602022                               India GDP 1960-2022                                   1KB  2022-11-11 12:08:46            619         25  1.0              \n",
      "whenamancodes/predict-diabities                                Predict Diabetes                                      9KB  2022-11-09 12:18:49           1816         41  1.0              \n",
      "elmoallistair/fertility-rate-19602020                          Fertility Rate (1960-2020)                           19KB  2022-11-17 01:31:16            345         23  0.9117647        \n",
      "zvr842/global-pollution-by-counties                            Global pollution by counties                         15KB  2022-11-14 10:57:31            613         24  0.9705882        \n",
      "whenamancodes/credit-card-customers-prediction                 Credit Card Customers Prediction                    379KB  2022-10-30 13:03:27           2729         61  1.0              \n",
      "whenamancodes/covid-19-coronavirus-pandemic-dataset            COVID -19 Coronavirus Pandemic Dataset               11KB  2022-09-30 04:05:11          11773        368  1.0              \n",
      "jillanisofttech/flight-price-prediction-dataset                Flight Price Prediction DataSet                     602KB  2022-11-18 10:37:29            690         26  0.7941176        \n",
      "ifteshanajnin/carinsuranceclaimprediction-classification       Car Insurance Claim Prediction                        2MB  2022-11-14 15:38:26            497         24  1.0              \n",
      "dbarteaux99/stable-diffusion-1-5                               Stable Diffusion 1.5 (normal and EMAonly) with vae    7GB  2022-10-23 15:40:29             41         15  0.9375           \n",
      "maharshipandya/-spotify-tracks-dataset                         ðŸŽ¹ Spotify Tracks Dataset                              8MB  2022-10-22 14:40:15           3436        100  1.0              \n",
      "aneesayoub/world-universities-ranking-2022                     World Universities Ranking 2022                      41KB  2022-11-17 03:47:37           1172         33  0.9117647        \n",
      "hasibalmuzdadid/global-air-pollution-dataset                   Global Air Pollution Dataset                        371KB  2022-11-08 14:43:32           2032         56  1.0              \n",
      "thedevastator/cancer-patients-and-air-pollution-a-new-link      Lung Cancer Prediction                               7KB  2022-11-14 13:40:40            758         29  1.0              \n",
      "salmankhaliq22/road-traffic-collision-dataset                  UK Road Traffic Collision Dataset                    99MB  2022-11-09 06:46:16            535         32  1.0              \n",
      "thedevastator/top-selling-nintendo-entertainment-system-games  Nintendo Entertainment System Games                  23KB  2022-11-11 17:47:38            494         22  0.85294116       \n"
     ]
    }
   ],
   "source": [
    "!pip install -q kaggle\n",
    "from google.colab import files\n",
    "files.upload()\n",
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "! kaggle datasets list"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "5mBqDUzfCTjY",
    "outputId": "3846b423-f562-48a7-a33e-39ef5f602d1e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading garbage-classification.zip to /content/sample_data\n",
      " 99% 236M/239M [00:02<00:00, 108MB/s]\n",
      "100% 239M/239M [00:02<00:00, 95.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d mostafaabla/garbage-classification -p /content/sample_data/ --unzip"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "LsJoZcOBCTjZ",
    "outputId": "dc14d989-34c7-4f49-dce2-6911dfa7e27e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#data processing\n",
    "def read_and_resize(filename, grayscale = False, fx= 1, fy=1):\n",
    "    if grayscale:\n",
    "        img_result = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "    else:\n",
    "        imgbgr = cv2.imread(filename, cv2.IMREAD_COLOR)\n",
    "        img_result = cv2.cvtColor(imgbgr, cv2.COLOR_BGR2RGB)\n",
    "    img_result = cv2.resize(img_result, None, fx=fx, fy=fy, interpolation = cv2.INTER_CUBIC)\n",
    "    return img_result\n",
    "\n",
    "#show images\n",
    "def show_in_row(list_of_images, titles = None, disable_ticks = False):\n",
    "    count = len(list_of_images)\n",
    "    for idx in range(count):\n",
    "        subplot = plt.subplot(1, count, idx+1)\n",
    "        if titles is not None:\n",
    "            subplot.set_title(titles[idx])\n",
    "\n",
    "        img = list_of_images[idx]\n",
    "        cmap = 'gray' if (len(img.shape) == 2 or img.shape[2] == 1) else None\n",
    "        subplot.imshow(img, cmap=cmap)\n",
    "        if disable_ticks:\n",
    "            plt.xticks([]), plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "#open txt file using path to the file and file_name.txt\n",
    "def openWithTxt(txt: str, path:str):\n",
    "    outData = []\n",
    "    with open(\"/content/sample_data/\"+txt) as f:\n",
    "        for i in f.readlines():\n",
    "            img ,classNum = i.split()\n",
    "            classNum = int(classNum)\n",
    "            img = read_and_resize(path+f\"{deIdx[classNum]}/\"+img).astype(\"float32\")/255\n",
    "            s = img.shape\n",
    "            img = cv2.resize(img, (s[0],s[0]), interpolation = cv2.INTER_AREA)\n",
    "            #outData.extend(rotate2sides(img, classNum))\n",
    "            outData.append((img, classNum))\n",
    "    return np.array(outData)\n",
    "\n",
    "def openWith(dfX, dfY):\n",
    "    outData = []\n",
    "    for i in range(len(dfX)):\n",
    "        path = \"./sample_data/garbageAll/\"\n",
    "        img = dfX[i]\n",
    "        classNum = dfY[i]\n",
    "        img = read_and_resize(path+img).astype(\"float32\")/255\n",
    "        img = cv2.resize(img, (256,256), interpolation = cv2.INTER_AREA)\n",
    "        outData.append((img, classNum))\n",
    "    return np.array(outData)\n",
    "\n",
    "# takes image ans its classNumber, and produce 4 rotaded images on exact degree (0, 90, 180, 270)\n",
    "def rotate4sides(img, classNum):\n",
    "    outData = []\n",
    "    outData.append(np.array([img, classNum]))\n",
    "    outData.append(np.array([cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE), classNum]))\n",
    "    outData.append(np.array([cv2.rotate(img, cv2.ROTATE_180), classNum]))\n",
    "    outData.append(np.array([cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE), classNum]))\n",
    "    return outData\n",
    "\n",
    "def rotate2sides(img, classNum):\n",
    "    outData = []\n",
    "    outData.append(np.array([img, classNum]))\n",
    "    outData.append(np.array([cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE), classNum]))\n",
    "    return outData"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def moveToAll(neededDirs):\n",
    "    pathTo   = \"./sample_data/garbageAll/\"\n",
    "    pathFrom = \"./sample_data/garbage_classification/\"\n",
    "\n",
    "    if not os.path.isdir(pathTo):\n",
    "        os.mkdir(pathTo)\n",
    "    else:\n",
    "        shutil.rmtree(pathTo)\n",
    "        os.mkdir(pathTo)\n",
    "\n",
    "    for folder in neededDirs:\n",
    "        print(len(os.listdir(pathFrom+folder)))\n",
    "        for filename in os.listdir(pathFrom+folder):\n",
    "            shutil.copy2(pathFrom+folder+\"/\"+filename, pathTo)\n",
    "\n",
    "neededDirs = [\"brown-glass\", \"cardboard\", \"green-glass\", \"plastic\", \"paper\", \"white-glass\", \"metal\"]\n",
    "moveToAll(neededDirs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#upload .csv in a folder\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"preprocessed_data.csv\")\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "classDecode = {1:\"paper/cardboard\", 2: \"metal\", 3: \"plastic\", 4: \"glass\"}\n",
    "\n",
    "x, x_test, y, y_test = train_test_split(df[\"photo\"],df[\"class\"],test_size=0.2,train_size=0.8)\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(x,y,test_size = 0.20,train_size =0.80)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_test.reset_index(inplace=True, drop=True)\n",
    "y_test.reset_index(inplace=True, drop=True)\n",
    "x_train.reset_index(inplace=True, drop=True)\n",
    "x_cv.reset_index(inplace=True, drop=True)\n",
    "y_train.reset_index(inplace=True, drop=True)\n",
    "y_cv.reset_index(inplace=True, drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "testData = openWith(x_test, y_test)\n",
    "xTest = np.stack(testData[:,0])\n",
    "yTest =  to_categorical(testData[:,1],num_classes = len(classDecode.values())+1)[:,1:]\n",
    "testData.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainData = openWith(x_train, y_train)\n",
    "xTrain = np.stack(trainData[:,0])\n",
    "yTrain = to_categorical(trainData[:,1],num_classes = len(classDecode.values())+1)[:,1:]\n",
    "trainData.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "valData = openWith(x_cv, y_cv)\n",
    "xVal = np.stack(valData[:,0])\n",
    "yVal = to_categorical(valData[:,1],num_classes = len(classDecode.values())+1)[:,1:]\n",
    "valData.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "yTrain.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(len(x_test)):\n",
    "    path = \"./sample_data/garbageAll/\"\n",
    "    img = x_test[i]\n",
    "    img = read_and_resize(path+img).astype(\"float32\")/255\n",
    "    img = cv2.resize(img, (256,256), interpolation = cv2.INTER_AREA)\n",
    "    show_in_row([img])\n",
    "    break\n",
    "\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "show_in_row(xTest[:5], yTest[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# model pytorch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#output - vector of vectors, labels - vactor\n",
    "def accuracy(outputs, labels):\n",
    "    return torch.tensor(torch.sum(outputs.argmax(1) == labels.argmax(1)).item() / len(labels))\n",
    "\n",
    "#implement Base for Classification\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        #print(batch.shape)\n",
    "        #print()\n",
    "        #batch - (img, label)\n",
    "        self.train() \n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        #batch - (img, label)\n",
    "        self.eval() \n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        #outputs - list of dicts of validations \n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(f\"Epoch {epoch+1}: train_loss: {result['train_loss']}, val_loss: {result['val_loss']}, val_acc: {result['val_acc']}\")\n",
    "\n",
    "class ImageClassificationInception(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        #print(batch.shape)\n",
    "        #print()\n",
    "        #batch - (img, label)\n",
    "        self.train() \n",
    "        images, labels = batch \n",
    "        outputs = self(images) \n",
    "        loss1 = F.cross_entropy(outputs, labels)\n",
    "        loss = loss1\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        #batch - (img, label)\n",
    "        self.eval() \n",
    "        images, labels = batch \n",
    "        outputs = self(images) \n",
    "        loss1 = F.cross_entropy(outputs, labels)\n",
    "        loss = loss1\n",
    "        acc = accuracy(outputs, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        #outputs - list of dicts of validations \n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(f\"Epoch {epoch+1}: train_loss: {result['train_loss']}, val_loss: {result['val_loss']}, val_acc: {result['val_acc']}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class ResNet(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Use a pretrained model\n",
    "        self.network = models.resnet50(pretrained=True)\n",
    "        # Replace last layer\n",
    "        num_ftrs = self.network.fc.in_features\n",
    "        self.network.fc = nn.Linear(num_ftrs, len(classDecode.values()))\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        o = self.network(xb)\n",
    "        return F.softmax(o)\n",
    "\n",
    "class DenseNet(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = models.densenet121(pretrained=True)\n",
    "        self.network.classifier = nn.Linear(1024, len(classDecode.values()))\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        o = self.network(xb)\n",
    "        return F.softmax(o)\n",
    "\n",
    "class InceptionNet(ImageClassificationInception):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = models.inception_v3(pretrained=True)\n",
    "        self.network.aux_logits=False\n",
    "        num_ftrs = self.network.AuxLogits.fc.in_features\n",
    "        self.network.AuxLogits.fc = nn.Linear(num_ftrs, len(classDecode.values()))\n",
    "        num_ftrs = self.network.fc.in_features\n",
    "        self.network.fc = nn.Linear(num_ftrs,len(classDecode.values()))\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        o = self.network(xb)\n",
    "        return F.softmax(o)\n",
    "\n",
    "\n",
    "class VGGNet(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Use a pretrained model\n",
    "        self.network = models.vgg16(pretrained=True)\n",
    "        # Replace last layer\n",
    "        self.network.classifier[6] = torch.nn.Linear(4096, len(classDecode.values()))\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        o = self.network(xb)\n",
    "        return F.softmax(o)\n",
    "\n",
    "class Net(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=(1,1))\n",
    "        self.conv2 = nn.Conv2d(16, 16, 3, padding=(1,1))\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3, padding=(1,1))\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(64, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4 , 128)\n",
    "        self.fc3 = nn.Linear(128, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = self.pool(F.relu(self.conv5(x)))\n",
    "        x = self.pool(F.relu(self.conv6(x)))\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, x, device, len):\n",
    "        self.dl = x\n",
    "        self.device = device\n",
    "        self.len = len\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for item in self.dl: \n",
    "            yield to_device(item, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class MyData(Dataset):\n",
    "    def __init__(self, x, y, transform):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "        self.len = len(x)\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    def __getitem__(self, item):\n",
    "        #print(type(self.x[item]))\n",
    "        return self.transform(self.x[item]), torch.from_numpy(self.y[item])\n",
    "\n",
    "device = get_default_device()\n",
    "device\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "batch_size = 32\n",
    "\n",
    "print(xTrain.shape)\n",
    "print(yTrain.shape)\n",
    "\n",
    "\n",
    "myDataTrain = MyData(xTrain,yTrain, transform)\n",
    "myDataVal = MyData(xVal,yVal, transform)\n",
    "myDataTest = MyData(xTest,yTest, transform)\n",
    "\n",
    "dataTrain = torch.utils.data.DataLoader(myDataTrain, batch_size=batch_size)\n",
    "dataVal = torch.utils.data.DataLoader(myDataVal, batch_size=batch_size)\n",
    "dataTest = torch.utils.data.DataLoader(myDataTest, batch_size=batch_size)\n",
    "\n",
    "dataTrain = DeviceDataLoader(dataTrain, device,len(yTrain))\n",
    "dataVal = DeviceDataLoader(dataVal, device,len(yVal))\n",
    "dataTest = DeviceDataLoader(dataTest, device,len(yTest))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inception"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = to_device(InceptionNet(), device)\n",
    "evaluate(model, dataTest)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 5.5e-5\n",
    "\n",
    "history = fit(num_epochs, lr, model, dataTrain, dataVal, opt_func)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluate(model, dataTest)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DenseNet\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = to_device(DenseNet(), device)\n",
    "evaluate(model, dataTest)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 5.5e-5 \n",
    "\n",
    "history = fit(num_epochs, lr, model, dataTrain, dataVal, opt_func)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluate(model, dataTest)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ResNet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = to_device(ResNet(), device)\n",
    "evaluate(model, dataTest)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 5.5e-5 \n",
    "\n",
    "history = fit(num_epochs, lr, model, dataTrain, dataVal, opt_func)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluate(model, dataTest)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# VGG\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model3 = to_device(VGGNet(), device)\n",
    "evaluate(model3, dataTest)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 5.5e-5 \n",
    "\n",
    "history = fit(num_epochs, lr, model3, dataTrain, dataVal, opt_func)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluate(model3, dataTest)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MyLittleModel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model2 = to_device(Net(), device)\n",
    "evaluate(model2, dataTest)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 5.5e-5 \n",
    "\n",
    "history = fit(num_epochs, lr, model2, dataTrain, dataVal, opt_func)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluate(model2, dataTest)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "EI7eGhdeP1Gf"
   ]
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "df9de60ad39f4255a596f2795c6ce779": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4b9ef7b515c34d179cbdcb4e5d257e1a",
       "IPY_MODEL_27b23169b855496992691357baef2be3",
       "IPY_MODEL_b7465566f64b45398d246d557662c508"
      ],
      "layout": "IPY_MODEL_f6dee2c4aca1421ba9e463cbff32410d"
     }
    },
    "4b9ef7b515c34d179cbdcb4e5d257e1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8ad90a9ad5b48fbac09415b8dc12078",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1908f939f85a44b397869894818ee7d0",
      "value": "100%"
     }
    },
    "27b23169b855496992691357baef2be3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_afe773d0fa064ee6b7ca1139d6129cb2",
      "max": 108949747,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_876fa66e69534de0b9d99fa8abd75900",
      "value": 108949747
     }
    },
    "b7465566f64b45398d246d557662c508": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ff642c3c2ec4ea6a922a66eedde7ef8",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c45cdc3876ed42e8a4cd97a1cae2b5d6",
      "value": " 104M/104M [00:00&lt;00:00, 198MB/s]"
     }
    },
    "f6dee2c4aca1421ba9e463cbff32410d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8ad90a9ad5b48fbac09415b8dc12078": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1908f939f85a44b397869894818ee7d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "afe773d0fa064ee6b7ca1139d6129cb2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "876fa66e69534de0b9d99fa8abd75900": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1ff642c3c2ec4ea6a922a66eedde7ef8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c45cdc3876ed42e8a4cd97a1cae2b5d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}