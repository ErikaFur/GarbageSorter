{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7LnIAzec9PJ8pqTfNEleZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vd2B6t6MEUCS",
        "outputId": "03c77965-7eef-436f-984f-eb258d04fd1c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Architecture"
      ],
      "metadata": {
        "id": "qH2D0v5gEqTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "\n",
        "#output - vector of vectors, labels - vactor\n",
        "def accuracy(outputs, labels):\n",
        "    return torch.tensor(torch.sum(outputs.argmax(1) == labels.argmax(1)).item() / len(labels))\n",
        "\n",
        "#implement Base for Classification\n",
        "class ImageClassificationBase(nn.Module):\n",
        "    def training_step(self, batch):\n",
        "        #print(batch.shape)\n",
        "        #print()\n",
        "        #batch - (img, label)\n",
        "        self.train()\n",
        "        images, labels = batch \n",
        "        out = self(images)                  # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        #batch - (img, label)\n",
        "        self.eval() \n",
        "        images, labels = batch \n",
        "        out = self(images)                    # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
        "        acc = accuracy(out, labels)           # Calculate accuracy\n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
        "        \n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        #outputs - list of dicts of validations \n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(f\"Epoch {epoch+1}: train_loss: {result['train_loss']}, val_loss: {result['val_loss']}, val_acc: {result['val_acc']}\")\n",
        "\n",
        "class ImageClassificationInception(nn.Module):\n",
        "    def training_step(self, batch):\n",
        "        #print(batch.shape)\n",
        "        #print()\n",
        "        #batch - (img, label)\n",
        "        self.train() \n",
        "        images, labels = batch \n",
        "        outputs = self(images) \n",
        "        loss1 = F.cross_entropy(outputs, labels)\n",
        "        loss = loss1\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        #batch - (img, label)\n",
        "        self.eval() \n",
        "        images, labels = batch \n",
        "        outputs = self(images) \n",
        "        loss1 = F.cross_entropy(outputs, labels)\n",
        "        loss = loss1\n",
        "        acc = accuracy(outputs, labels)           # Calculate accuracy\n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
        "        \n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        #outputs - list of dicts of validations \n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(f\"Epoch {epoch+1}: train_loss: {result['train_loss']}, val_loss: {result['val_loss']}, val_acc: {result['val_acc']}\")"
      ],
      "metadata": {
        "id": "F2GE7z5FElOP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classDecode = {1:\"paper/cardboard\", 2: \"metal\", 3: \"plastic\", 4: \"glass\"}\n",
        "class ResNet(ImageClassificationBase):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Use a pretrained model\n",
        "        self.network = models.resnet50(pretrained=True)\n",
        "        # Replace last layer\n",
        "        num_ftrs = self.network.fc.in_features\n",
        "        self.network.fc = nn.Linear(num_ftrs, len(classDecode.values()))\n",
        "    \n",
        "    def forward(self, xb):\n",
        "        o = self.network(xb)\n",
        "        return F.softmax(o)\n",
        "\n",
        "class DenseNet(ImageClassificationBase):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.network = models.densenet121(pretrained=True)\n",
        "        self.network.classifier = nn.Linear(1024, len(classDecode.values()))\n",
        "    \n",
        "    def forward(self, xb):\n",
        "        o = self.network(xb)\n",
        "        return F.softmax(o)\n",
        "\n",
        "class InceptionNet(ImageClassificationInception):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.network = models.inception_v3(pretrained=True)\n",
        "        self.network.aux_logits=False\n",
        "        num_ftrs = self.network.AuxLogits.fc.in_features\n",
        "        self.network.AuxLogits.fc = nn.Linear(num_ftrs, len(classDecode.values()))\n",
        "        num_ftrs = self.network.fc.in_features\n",
        "        self.network.fc = nn.Linear(num_ftrs,len(classDecode.values()))\n",
        "    \n",
        "    def forward(self, xb):\n",
        "        o = self.network(xb)\n",
        "        return F.softmax(o)\n",
        "\n",
        "\n",
        "class VGGNet(ImageClassificationBase):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Use a pretrained model\n",
        "        self.network = models.vgg16(pretrained=True)\n",
        "        # Replace last layer\n",
        "        self.network.classifier[6] = torch.nn.Linear(4096, len(classDecode.values()))\n",
        "    \n",
        "    def forward(self, xb):\n",
        "        o = self.network(xb)\n",
        "        return F.softmax(o)\n",
        "\n",
        "class Net(ImageClassificationBase):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=(1,1))\n",
        "        self.conv2 = nn.Conv2d(16, 16, 3, padding=(1,1))\n",
        "        self.conv3 = nn.Conv2d(16, 32, 3, padding=(1,1))\n",
        "        self.conv4 = nn.Conv2d(32, 32, 3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv6 = nn.Conv2d(64, 32, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc1 = nn.Linear(32 * 4 * 4 , 128)\n",
        "        self.fc3 = nn.Linear(128, 6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = self.pool(F.relu(self.conv4(x)))\n",
        "        x = self.pool(F.relu(self.conv5(x)))\n",
        "        x = self.pool(F.relu(self.conv6(x)))\n",
        "        #print(x.shape)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc3(x)\n",
        "        return F.softmax(x)\n"
      ],
      "metadata": {
        "id": "Q36t2RysEocw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model loading"
      ],
      "metadata": {
        "id": "Q5UnqbnjEuGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_default_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "def to_device(data, device):\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n"
      ],
      "metadata": {
        "id": "CdxTtCkAE0Pt"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import os.path\n",
        "\n",
        "def read_and_resize(filename, grayscale = False, fx= 1, fy=1):\n",
        "    #read file\n",
        "    if grayscale:\n",
        "        img_result = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
        "    else:\n",
        "        imgbgr = cv2.imread(filename, cv2.IMREAD_COLOR)\n",
        "        img_result = cv2.cvtColor(imgbgr, cv2.COLOR_BGR2RGB)\n",
        "    img_result = cv2.resize(img_result, None, fx=fx, fy=fy, interpolation = cv2.INTER_CUBIC)\n",
        "    return img_result\n",
        "\n",
        "def getImageFromFolderAndMove(pathToImage, pathNewFolderImages):\n",
        "    #get image and move produced image to another folder\n",
        "    out = []\n",
        "    if not os.path.isdir(pathNewFolderImages):\n",
        "        os.mkdir(pathNewFolderImages)\n",
        "    if len(os.listdir(pathToImage)) == 0:\n",
        "        print(\"Folder toSort is empty!\")\n",
        "    for filename in os.listdir(pathToImage):\n",
        "        imgPath = (pathToImage+\"/\").replace(\"//\", \"/\")+filename\n",
        "        out = read_and_resize(imgPath)\n",
        "        shutil.move(imgPath, (pathNewFolderImages+\"/\").replace(\"//\", \"/\")+filename)\n",
        "        return [out, filename]\n",
        "\n",
        "def resizeImage(img, size):\n",
        "    #reshape to the shape of (size, size)\n",
        "    return cv2.resize(img, (256,256), interpolation = cv2.INTER_AREA).astype(\"float32\")/255\n",
        "\n",
        "def convertImg(img, device):\n",
        "    #prepare image to pass it to the model\n",
        "    return torch.tensor([img.swapaxes(2,1).swapaxes(1,0)]).to(device)\n",
        "\n",
        "def passToModel(model, image):\n",
        "    return model(image).argmax(1)\n",
        "\n",
        "def loadModel(pathTo, device):\n",
        "    model = to_device(DenseNet(), device)\n",
        "    model.load_state_dict(torch.load(pathTo, map_location=device))\n",
        "    return model\n",
        "\n",
        "def logInfo(pathToCsv, imgName, predict, predictClass):\n",
        "    if not os.path.isfile(pathToCsv): \n",
        "        df = pd.DataFrame({'name': [],\n",
        "                   'class': [],\n",
        "                   'classname': []})\n",
        "    else:\n",
        "        df = pd.read_csv(pathToCsv)\n",
        "    new_row = pd.DataFrame({'name': [imgName],\n",
        "                   'class': [predict],\n",
        "                   'classname': [predictClass]})\n",
        "    df = df.append(new_row, ignore_index=True)\n",
        "    df.to_csv(pathToCsv, index=False)\n",
        "\n",
        "\n",
        "def getResizePass(pathToModel, pathToImage, pathNewFolderImages, csvFilePath, classDecode):\n",
        "    device = get_default_device()\n",
        "    model = loadModel(pathToModel, device)\n",
        "    img, filename = getImageFromFolderAndMove(pathToImage, pathNewFolderImages)\n",
        "    resImg = convertImg(resizeImage(img, 256), device)\n",
        "\n",
        "    out = passToModel(model, resImg)\n",
        "    print(out)\n",
        "    cls = classDecode[int(out[0]+1)]\n",
        "    numCls = int(out[0]+1)\n",
        "    log = logInfo(csvFilePath, filename, numCls, cls)\n",
        "    print(f\"Model predicted class {numCls}, which is a {cls}\")\n",
        "    \n",
        "def main():\n",
        "    pathToModel = \"./drive/MyDrive/denceNetModel.pt\" #pretrained model. It takes RGB images with size 256x256\n",
        "    imgPath = \"./drive/MyDrive/toSort\" #folder with all images to sort\n",
        "    imgNewPath = \"./drive/MyDrive/sorted\" #folder with all sorted images \n",
        "    csvPath = \"./drive/MyDrive/logGarbage.csv\" #file for the logging\n",
        "    classDecoder = {1:\"paper/cardboard\", 2: \"metal\", 3: \"plastic\", 4: \"glass\"} #all classes\n",
        "    \n",
        "    getResizePass(pathToModel, imgPath, imgNewPath, csvPath, classDecoder)"
      ],
      "metadata": {
        "id": "4W4E1qXtFk7D"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "jkmmi6sZEQgS",
        "outputId": "15c57fdb-c872-4271-e16b-f280dd780ed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder toSort is empty!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-125-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-124-807e07cb9b58>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mclassDecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"paper/cardboard\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"metal\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"plastic\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"glass\"\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m#all classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mgetResizePass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathToModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgNewPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsvPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassDecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-124-807e07cb9b58>\u001b[0m in \u001b[0;36mgetResizePass\u001b[0;34m(pathToModel, pathToImage, pathNewFolderImages, csvFilePath, classDecode)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_default_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathToModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetImageFromFolderAndMove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathToImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathNewFolderImages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mresImg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvertImg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresizeImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
          ]
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "iMpIjwBjMi2w"
      },
      "execution_count": 98,
      "outputs": []
    }
  ]
}